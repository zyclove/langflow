"use strict";(self.webpackChunklangflow_docs=self.webpackChunklangflow_docs||[]).push([[4806],{17886:(e,n,o)=>{o.d(n,{A:()=>t});o(96540);var s=o(64058),i=o(74848);function t({name:e,...n}){const o=s[e];return o?(0,i.jsx)(o,{...n}):null}},28453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>l});var s=o(96540);const i={},t=s.createContext(i);function r(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(t.Provider,{value:n},e.children)}},88322:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>a,contentTitle:()=>c,default:()=>p,frontMatter:()=>l,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"Concepts/concepts-voice-mode","title":"Voice mode","description":"The Langflow Playground supports voice mode for interacting with your applications through a microphone.","source":"@site/docs/Concepts/concepts-voice-mode.md","sourceDirName":"Concepts","slug":"/concepts-voice-mode","permalink":"/concepts-voice-mode","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"Voice mode","slug":"/concepts-voice-mode"},"sidebar":"docs","previous":{"title":"Manage files","permalink":"/concepts-file-management"},"next":{"title":"Agents","permalink":"/components-agents"}}');var i=o(74848),t=o(28453),r=o(17886);const l={title:"Voice mode",slug:"/concepts-voice-mode"},c=void 0,a={},d=[{value:"Prerequisite",id:"prerequisite",level:2},{value:"Use voice mode in the Langflow Playground",id:"use-voice-mode-in-the-langflow-playground",level:2},{value:"Langflow voice mode endpoints",id:"langflow-voice-mode-endpoints",level:2}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:["The Langflow ",(0,i.jsx)(n.strong,{children:"Playground"})," supports ",(0,i.jsx)(n.strong,{children:"voice mode"})," for interacting with your applications through a microphone."]}),"\n",(0,i.jsxs)(n.p,{children:["An ",(0,i.jsx)(n.a,{href:"https://platform.openai.com/",children:"OpenAI API key"})," is required to use ",(0,i.jsx)(n.strong,{children:"voice mode"}),". An ",(0,i.jsx)(n.a,{href:"https://elevenlabs.io",children:"ElevenLabs"})," API key enables more voices in the chat, but is optional."]}),"\n",(0,i.jsxs)(n.p,{children:["Your flow must have a ",(0,i.jsx)(n.a,{href:"/components-io#chat-input",children:"Chat input"})," component to interact with the ",(0,i.jsx)(n.strong,{children:"Playground"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisite",children:"Prerequisite"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://platform.openai.com/",children:"An OpenAI API key"})}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"use-voice-mode-in-the-langflow-playground",children:"Use voice mode in the Langflow Playground"}),"\n",(0,i.jsxs)(n.p,{children:["Chat with an agent in the ",(0,i.jsx)(n.strong,{children:"Playground"}),", and get more recent results by asking the agent to use tools."]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Create a ",(0,i.jsx)(n.a,{href:"/simple-agent",children:"Simple agent starter project"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Add your ",(0,i.jsx)(n.strong,{children:"OpenAI API key"})," credentials to the ",(0,i.jsx)(n.strong,{children:"Agent"})," component."]}),"\n",(0,i.jsxs)(n.li,{children:["To start a chat session, click ",(0,i.jsx)(n.strong,{children:"Playground"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["To enable voice mode, click the ",(0,i.jsx)(r.A,{name:"Mic","aria-label":"Microphone"})," icon.\nThe ",(0,i.jsx)(n.strong,{children:"Voice mode"})," pane opens."]}),"\n",(0,i.jsxs)(n.li,{children:["In the ",(0,i.jsx)(n.strong,{children:"OpenAI API Key"})," field, add your ",(0,i.jsx)(n.strong,{children:"OpenAI API key"})," credentials.\nThis key is saved as a ",(0,i.jsx)(n.a,{href:"/configuration-global-variables",children:"global variable"})," in Langflow and is accessible from any component or flow."]}),"\n",(0,i.jsxs)(n.li,{children:["Your browser may prompt you for microphone access.\nBrowser access is ",(0,i.jsx)(n.strong,{children:"required"})," to use voice mode.\nTo continue, allow microphone access in your browser."]}),"\n",(0,i.jsxs)(n.li,{children:["In the ",(0,i.jsx)(n.strong,{children:"Audio Input"})," menu, select the input device to use with voice mode."]}),"\n"]}),"\n",(0,i.jsx)(n.admonition,{type:"tip",children:(0,i.jsx)(n.p,{children:"A higher quality microphone improves OpenAI's voice chat comprehension."})}),"\n",(0,i.jsxs)(n.ol,{start:"8",children:["\n",(0,i.jsxs)(n.li,{children:["Optionally, add your ",(0,i.jsx)(n.strong,{children:"ElevenLabs API key"})," in the ",(0,i.jsx)(n.strong,{children:"ElevenLabs API Key"})," field.\nThis makes more voices available for your AI responses.\nThis key is saved as a ",(0,i.jsx)(n.a,{href:"/configuration-global-variables",children:"global variable"})," in Langflow and is accessible from any component or flow."]}),"\n",(0,i.jsxs)(n.li,{children:["In the ",(0,i.jsx)(n.strong,{children:"Preferred Language"})," menu, select your language for conversing with Langflow.\nThis option changes both the spoken conversation and the chat responses in the ",(0,i.jsx)(n.strong,{children:"Playground"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Talk into your microphone.\nThe waveform in the voice mode pane should register your input, and the agent should respond in voice and in the ",(0,i.jsx)(n.strong,{children:"Playground"}),"."]}),"\n",(0,i.jsx)(n.li,{children:"Ask the agent to use the tools available to find recent news about a subject."}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["The agent describes its search process, including accessing the ",(0,i.jsx)(n.strong,{children:"URL"})," tool to fetch recent news.\nThe agent summarizes the recent news in speech and in the ",(0,i.jsx)(n.strong,{children:"Playground"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"Be aware of the following considerations when using voice mode:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Name and describe your tools accurately, so the ",(0,i.jsx)(n.strong,{children:"Agent"})," chooses tools correctly."]}),"\n",(0,i.jsxs)(n.li,{children:["Voice mode does not use the instructions in the Agent component's ",(0,i.jsx)(n.strong,{children:"Agent Instructions"})," field, because your spoken instructions override this value."]}),"\n",(0,i.jsxs)(n.li,{children:["Voice mode only maintains context within the conversation session you are currently in.\nIf you exit a conversation and close the ",(0,i.jsx)(n.strong,{children:"Playground"}),", your conversational context is not available in the next chat session."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"langflow-voice-mode-endpoints",children:"Langflow voice mode endpoints"}),"\n",(0,i.jsxs)(n.p,{children:["Langflow exposes OpenAI Realtime API-compatible websocket endpoints for your flows. You can build voice applications against these endpoints the same way you would build against ",(0,i.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/realtime#connect-with-websockets",children:"OpenAI Realtime API websockets"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["The WebSockets endpoints require an ",(0,i.jsx)(n.a,{href:"https://platform.openai.com/docs/overview",children:"OpenAI API key"})," for authentication, and they support an optional ",(0,i.jsx)(n.a,{href:"https://elevenlabs.io",children:"ElevenLabs"})," integration."]}),"\n",(0,i.jsx)(n.p,{children:"Langflow exposes two WebSockets endpoints:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"/ws/flow_as_tool/{flow_id}"})," or ",(0,i.jsx)(n.code,{children:"/ws/flow_as_tool/{flow_id}/{session_id}"}),": Establishes a connection to OpenAI Realtime voice, and then invokes flows as tools by the ",(0,i.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/realtime-conversations#handling-audio-with-websockets",children:"OpenAI Realtime model"}),".\nThis approach is ideal for low latency applications, but it is less deterministic since the OpenAI voice-to-voice model determines when to call your flow."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"/ws/flow_tts/{flow_id}"})," or ",(0,i.jsx)(n.code,{children:"/ws/flow_tts/{flow_id}/{session_id}"}),": Converts audio to text using ",(0,i.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/realtime-transcription",children:"OpenAI Realtime voice transcription"}),", and then each flow is invoked directly for each transcript.\nThis approach is more deterministic but has higher latency.\nThis is the mode used in the Langflow playground."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Path parameters:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"flow_id"}),": Required path parameter. The ID of the flow to be used as a tool."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"session_id"}),": Optional path parameter. A unique identifier for the conversation session. If not provided, one is automatically generated."]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}}}]);